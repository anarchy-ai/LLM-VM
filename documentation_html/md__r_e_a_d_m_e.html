<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>AnarchyDoc: README</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">AnarchyDoc
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('md__r_e_a_d_m_e.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">README</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><img src="anarchy_logo.svg" alt="Anarchy Logo" style="pointer-events: none;" class="inline"/></p>
<p align="center"></p>
<p><a href="https://anarchy.ai/" target="_blank"><img src="https://img.shields.io/badge/View%20Documentation-Docs-yellow" alt="" class="inline"/></a> <a href="https://discord.gg/YmNvCAk6W6" target="_blank"><img src="https://img.shields.io/badge/Join%20our%20community-Discord-blue" alt="" class="inline"/></a> <a href="https://github.com/anarchy-ai/LLM-VM"><img src="https://img.shields.io/github/stars/anarchy-ai/LLM-VM" alt="" class="inline"/> </a> </p>
<h1 align="center">ğŸ¤– Anarchy LLM-VM ğŸ¤– </h1>
<p align="center"></p>
<p><em>An Open-Source AGI Server for Open-Source LLMs</em></p>
<p>This is <a href="https://anarchy.ai">Anarchy's</a> effort to build ğŸ—ï¸ an open generalized artificial intelligence ğŸ¤– through the LLM-VM: a way to give your LLMs superpowers ğŸ¦¸ and superspeed ğŸš„.</p>
<p>You can find detailed instructions to try it live here: <a href="https://anarchy.ai">anarchy.ai</a></p>
<blockquote class="doxtable">
<p>&zwj;This project is in BETA. Expect continuous improvement and development. </p>
</blockquote>
<h1><a class="anchor" id="autotoc_md4"></a>
Table of Contents</h1>
<ul>
<li>Table of Contents</li>
<li>About<ul>
<li>What</li>
<li>Why</li>
<li>Features and Roadmap</li>
</ul>
</li>
<li>Quick Start and Installation<ul>
<li>Requirements</li>
<li>Installation</li>
<li>Generating Completions</li>
<li>Running LLMs Locally</li>
<li>Supported Models</li>
<li>Picking Different Models</li>
<li>Tool Usage</li>
</ul>
</li>
<li>Contributing</li>
</ul>
<h2><a class="anchor" id="autotoc_md5"></a>
ğŸ“š About ğŸ“š</h2>
<h3><a class="anchor" id="autotoc_md6"></a>
ğŸ’ What is the Anarchy LLM-VM?</h3>
<p>The Anarchy LLM-VM is a highly optimized and opinionated backend for running LLMs with all the modern features we've come to expect from completion: tool usage, persistent stateful memory, live data augmentation, data and task fine-tuning, output templating, a web playground, API endpoints, student-teacher distillation, data synthesis, load-balancing and orchestration, large context-window mimicry.</p>
<p>Formally, it is a virtual machine/interpreter for human language, coordinating between data, models (CPU), your prompts (code), and tools (IO).</p>
<p>By doing all these things in one spot in an opinionated way, the LLM-VM can properly optimize batch calls that would be exorbitantly expensive with distributed endpoints. It furthermore strives for both model and architecture agnosticism, properly optimizing the chosen model for the current architecture.</p>
<h3><a class="anchor" id="autotoc_md7"></a>
ğŸ¤Œ Why use the Anarchy LLM-VM?</h3>
<p>In line with Anarchy's mission, the LLM-VM strives to support open-source models. By utilizing open-source models and running them locally you achieve a number of benefits:</p>
<ul>
<li><b>Speed up your AGI development ğŸš€:</b> <em>With AnarchyAI, one interface is all you need to interact with the latest LLMs available.</em></li>
<li><b>Lower your costs ğŸ’¸:</b> <em>Running models locally can reduce the pay-as-you-go costs of development and testing.</em></li>
<li><b>Flexibility ğŸ§˜â€â™€ï¸:</b> <em>Anarchy allows you to rapidly switch between popular models so you can pinpoint the exact right tool for your project.</em></li>
<li><b>Community Vibes ğŸ«‚:</b> <em>Join our active community of highly motivated developers and engineers working passionately to democratize AGI</em></li>
<li><b>WYSIWYG ğŸ‘€:</b> <em>Open source means nothing is hidden; we strive for transparency and efficiency so you can focus on building.</em></li>
</ul>
<h3><a class="anchor" id="autotoc_md8"></a>
ğŸ Features and Roadmap</h3>
<ul>
<li><b>Implicit Agents ğŸ”§ğŸ•µï¸:</b> <em>The Anarchy LLM-VM can be set up to use external tools through our agents such as <b>REBEL</b> just by supplying tool descriptions!</em></li>
<li><b>Inference Optimization ğŸš„:</b> <em>The Anarchy LLM-VM is optimized from the agent level all the way to assembly on known LLM architectures to get the most bang for your buck. With state-of-the-art batching, sparse inference and quantization, distillation, and multi-level colocation, we aim to provide the fastest framework available.</em></li>
<li><b>Task Auto-Optimization ğŸš…:</b> <em>The Anarchy LLM-VM will analyze your use cases for repetitive tasks where it can activate student-teacher distillation to train a super-efficient small model from a larger more general model without losing accuracy. It can furthermore take advantage of data-synthesis techniques to improve results.</em></li>
<li><b>Library Callable ğŸ“š:</b> <em>We provide a library that can be used from any Python codebase directly.</em></li>
<li><b>HTTP Endpoints ğŸ•¸ï¸:</b> <em>We provide an HTTP standalone server to handle completion requests.</em></li>
<li><b>Live Data Augmentation ğŸ“Š:</b> (ROADMAP) <em>You will be able to provide a live updating data set and the Anarchy LLM-VM will <b>fine-tune</b> your models or work with a <b>vector DB</b> to provide up-to-date information with citations</em></li>
<li><b>Web Playground ğŸ›:</b> (ROADMAP) <em>You will be able to run the Anarchy LLM-VM and test its outputs from the browser.</em></li>
<li><b>Load-Balancing and Orchestration âš–ï¸:</b> (ROADMAP) <em>If you have multiple LLMs or providers you'd like to utilize, you will be able to hand them to the Anarchy LLM-VM to automatically figure out which to work with and when to optimize your uptime or your costs</em></li>
<li><b>Output Templating ğŸ¤µ:</b> (ROADMAP) <em>You can ensure that the LLM only outputs data in specific formats and fills in variables from a template with either regular expressions, LMQL, or OpenAI's template language</em></li>
<li><b>Persistent Stateful Memory ğŸ“:</b> (ROADMAP) <em>The Anarchy LLM-VM can remember a user's conversation history and react accordingly</em></li>
</ul>
<h2><a class="anchor" id="autotoc_md9"></a>
ğŸš€ Quickstart ğŸš€</h2>
<h3><a class="anchor" id="autotoc_md10"></a>
ğŸ¥¹ Requirements</h3>
<h4><a class="anchor" id="autotoc_md11"></a>
Installation Requirements</h4>
<p>Python &gt;=3.10 Supported. Older versions of Python are on a best-effort basis.</p>
<p>Use <code>bash &gt; python3 --version</code> to check what version you are on.</p>
<p>To upgrade your python, either create a new python env using <code>bash &gt; conda create -n myenv python=3.10</code> or go to <a href="https://www.python.org/downloads/">https://www.python.org/downloads/</a> to download the latest version.</p>
<h4><a class="anchor" id="autotoc_md12"></a>
System Requirements</h4>
<p>Different models have different system requirements. Limiting factors on most systems will likely be RAM, but many functions will work at even 16 GB of ram.</p>
<p>That said, always lookup information about the models you're using, they all have different sizes and requirements in memory and compute resources.</p>
<h3><a class="anchor" id="autotoc_md13"></a>
ğŸ‘¨â€ğŸ’» Installation</h3>
<p>To install the LLM-VM you simply need to download this repository and install it with pip like so:</p>
<div class="fragment"><div class="line">&gt; git clone https://github.com/anarchy-ai/LLM-VM.git</div>
<div class="line">&gt; cd LLM-VM</div>
<div class="line">&gt; pip3 install .</div>
</div><!-- fragment --> <h4><a class="anchor" id="autotoc_md14"></a>
Developer Setup</h4>
<p>the best way to test run code while writing a patch is to use <code>pip3 install --editable .</code>, which will expose the code as a library you can edit in place.</p>
<p>This will install both the library and test-server. <br  />
</p>
<p>And for installing dev dependencies, use <code>pip3 install -e ."[dev]"</code></p>
<h4><a class="anchor" id="autotoc_md15"></a>
One Last Step, almost there!</h4>
<p>If you're using one of the OpenAI models, you will need to set the <code>LLM_VM_OPENAI_API_KEY</code> environment variable with your API key.</p>
<h3><a class="anchor" id="autotoc_md16"></a>
âœ… Generating Completions</h3>
<p>Our LLM-VM gets you working directly with popular LLMs locally in just 3 lines. Once you've installed it (as above), just load your model and start generating!</p>
<div class="fragment"><div class="line"><span class="comment"># import our client</span></div>
<div class="line"><span class="keyword">from</span> llm_vm.client <span class="keyword">import</span> Client</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Select which LLM you want to use, here we have OpenAI&#39;s </span></div>
<div class="line">client = Client(big_model = <span class="stringliteral">&#39;chat_gpt&#39;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Put in your prompt and go!</span></div>
<div class="line">response = client.complete(prompt = <span class="stringliteral">&#39;What is Anarchy?&#39;</span>, context = <span class="stringliteral">&#39;&#39;</span>, openai_key = <span class="stringliteral">&#39;ENTER_YOUR_API_KEY&#39;</span>)</div>
<div class="line">print(response)</div>
<div class="line"><span class="comment"># Anarchy is a political ideology that advocates for the absence of government...</span></div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md17"></a>
ğŸƒâ€â™€ Running LLMs Locally</h3>
<div class="fragment"><div class="line"><span class="comment"># import our client</span></div>
<div class="line"><span class="keyword">from</span> llm_vm.client <span class="keyword">import</span> Client</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Select the LlaMA model</span></div>
<div class="line">client = Client(big_model = <span class="stringliteral">&#39;llama&#39;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Put in your prompt and go!</span></div>
<div class="line">response = client.complete(prompt = <span class="stringliteral">&#39;What is Anarchy?&#39;</span>, context = <span class="stringliteral">&#39;&#39;</span>)</div>
<div class="line">print(response)</div>
<div class="line"><span class="comment"># Anarchy is a political philosophy that advocates no government...</span></div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md18"></a>
ğŸ˜ Supported Models</h3>
<p>Select from the following models </p><div class="fragment"><div class="line">Supported_Models = [<span class="stringliteral">&#39;chat_gpt&#39;</span>,<span class="stringliteral">&#39;gpt&#39;</span>,<span class="stringliteral">&#39;neo&#39;</span>,<span class="stringliteral">&#39;llama&#39;</span>,<span class="stringliteral">&#39;bloom&#39;</span>,<span class="stringliteral">&#39;opt&#39;</span>,<span class="stringliteral">&#39;pythia&#39;</span>]</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md19"></a>
â˜¯ Picking Different Models</h3>
<p>LLM-VM default model sizes for local models are intended to make experimentation with LLMs accessible to everyone, but if you have the memory required, larger parameter models will perform far better!</p>
<p>for example, if you want to use a large and small neo model for your teacher and student, and you have enough ram:</p>
<div class="fragment"><div class="line"><span class="comment"># import our client</span></div>
<div class="line"><span class="keyword">from</span> llm_vm.client <span class="keyword">import</span> Client</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Select the LlaMA model</span></div>
<div class="line">client = Client(big_model = <span class="stringliteral">&#39;neo&#39;</span>, big_model_config={<span class="stringliteral">&#39;model_uri&#39;</span>:<span class="stringliteral">&#39;EleutherAI/gpt-neox-20b&#39;</span>}, </div>
<div class="line">                small_model =<span class="stringliteral">&#39;neo&#39;</span>, small_model_config={<span class="stringliteral">&#39;model_uri&#39;</span>:<span class="stringliteral">&#39;EleutherAI/gpt-neox-125m&#39;</span>})</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Put in your prompt and go!</span></div>
<div class="line">response = client.complete(prompt = <span class="stringliteral">&#39;What is Anarchy?&#39;</span>, context = <span class="stringliteral">&#39;&#39;</span>)</div>
<div class="line">print(response)</div>
<div class="line"><span class="comment"># Anarchy is a political philosophy that advocates no government...</span></div>
</div><!-- fragment --><p>For some other choices of memory usage and parameter count in each model family, check out the tables in <a class="el" href="md_model__uri__tables.html">model_uri_tables</a></p>
<h3><a class="anchor" id="autotoc_md20"></a>
ğŸ›  Tool Usage</h3>
<p>There are two agents: FLAT and REBEL.</p>
<p>Run the agents separately by going into the <code>src/llm_vm/agents/&lt;AGENT_FOLDER&gt;</code> and running the file that is titled <code>agent.py</code>.</p>
<p>Alternatively, to run a simple interface and choose an agent to run from the CLI, run the <code><a class="el" href="src_2llm__vm_2agents_2agent__interface_8py.html">src/llm_vm/agents/agent_interface.py</a></code> file and follow the command prompt instructions.</p>
<h2><a class="anchor" id="autotoc_md21"></a>
ğŸ©· Contributing ğŸ©·</h2>
<p>We welcome contributors! The best way to get started is to join our active <a href="https://discord.anarchy.ai">discord community</a>. Otherwise here are some ways to contribute and get paid:</p>
<h3><a class="anchor" id="autotoc_md22"></a>
Jobs</h3>
<ul>
<li>We're always looking for serious hackers. Prove that you can build and creatively solve hard problems and reach out!</li>
<li>The easiest way to secure a job/internship with us is to submit a pull request that closes a ticket.</li>
<li>The second easiest is to submit good tickets.</li>
<li>Otherwise, to apply directly: <a href="https://forms.gle/bUWDKW3cwZ8n6qsU8">https://forms.gle/bUWDKW3cwZ8n6qsU8</a></li>
</ul>
<h3><a class="anchor" id="autotoc_md23"></a>
Bounty</h3>
<p>Some tickets we'll pay you for closing! Look at the ticket labels to see how much the bounty is. To get started, <a href="https://discord.com/channels/1075227138766147656/1147542772824408074">join the discord and read the guide</a></p>
<h2><a class="anchor" id="autotoc_md24"></a>
ğŸ™ Acknowledgements ğŸ™</h2>
<ul>
<li><b>Matthew Mirman</b> - CEO<ul>
<li>GitHub: <a href="https://github.com/mmirman">@mmirman</a></li>
<li>LinkedIn: <a href="https://www.linkedin.com/in/matthewmirman/">@matthewmirman</a></li>
<li>Twitter: <a href="https://twitter.com/mmirman">@mmirman</a></li>
<li>Website: <a href="https://www.mirman.com">mirman.com</a></li>
</ul>
</li>
<li><b>Abhigya Sodani</b> - Research Intern<ul>
<li>GitHub: <a href="https://github.com/abhigya-sodani">@abhigya-sodani</a></li>
<li>LinkedIn: <a href="https://www.linkedin.com/in/abhigya-sodani-405918160/">@abhigya-sodani</a></li>
</ul>
</li>
<li><b>Carter Schonwald</b> - Community Engineering Leader<ul>
<li>GitHub: <a href="https://github.com/cartazio">@cartazio</a></li>
<li>LinkedIn: <a href="https://www.linkedin.com/in/carter-schonwald-aa178132/">@carter-schonwald</a></li>
</ul>
</li>
<li><b>Andrew Nelson</b> - Basilisk Trainer<ul>
<li>GitHub: <a href="https://github.com/ajn2004">@ajn2004</a></li>
<li>LinkedIn: <a href="https://www.linkedin.com/in/ajnelsnyc/">@ajnelsnyc</a></li>
</ul>
</li>
<li><b>Kyle Wild</b> - Undoomer<ul>
<li>GitHub: <a href="https://github.com/dorkitude">@dorkitude</a></li>
<li>LinkedIn: <a href="https://www.linkedin.com/in/kylewild/">@kylewild</a></li>
</ul>
</li>
<li><b>Victor Odede</b> - Fearless Contributor<ul>
<li>GitHub: <a href="https://github.com/VictorOdede">@VictorOdede</a></li>
<li>LinkedIn: <a href="https://www.linkedin.com/in/victor-odede-aaa907114/">@victor-odede</a></li>
</ul>
</li>
<li><b>Aarushi Banerjee</b> - Fearless Contributor<ul>
<li>GitHub: <a href="https://github.com/AB3000">@AB3000</a></li>
<li>LinkedIn: <a href="https://www.linkedin.com/in/ab99/">@ab99</a></li>
</ul>
</li>
</ul>
<h2><a class="anchor" id="autotoc_md25"></a>
License</h2>
<p>[MIT License](LICENSE) </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8 </li>
  </ul>
</div>
</body>
</html>
